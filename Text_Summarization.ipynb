{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m4rk00s/nlp-project/blob/master/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqYg2lDkR6eB",
        "colab_type": "text"
      },
      "source": [
        "# Extraction-based Text Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUCj0jkrSAc6",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB9u2s-iU9hU",
        "colab_type": "text"
      },
      "source": [
        "Text summarization is an act to generate a short text that describe the text wholly. Sometimes, text summarization is called automatic summarization, because the process is automated by a software.\n",
        "\n",
        "There's a number of way to build a text summarization, one particular is using **SumBasic** system. Here's the algorithm:\n",
        "\n",
        "1. Compute the probability of a word $w$\n",
        "   $$P(w) = \\frac{f(w)}{N}$$\n",
        "   where $f(w)$ is the number of occurences of the word, and $N$ is the number of all words in the input.\n",
        "   \n",
        "2. For each sentence, $S_j$, in the input, assigns a weight equal to the average probability of the words in the sentence:\n",
        "   $$g(S_j) =\\frac{\\sum_{w_i\\in S_j} P(w_i)}{|\\{w_i|w_i\\in S_j\\}|}$$\n",
        "   \n",
        "3. Pick the best scoring sentnece that contains the highest probability word.\n",
        "\n",
        "4. For each word in the chosen sentence, the weight is updated:\n",
        "   $$p_{\\text{new}}(w_i) = p_{\\text{old}}(w_i)p_{\\text{old}}(w_i)$$\n",
        "   This word weight update indicates that the probability of a word appearing in the summary is lower than a word occuring once.\n",
        "   \n",
        "5. If the desired summary length has not been reached, go back to step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpsZXol1U_Y5",
        "colab_type": "text"
      },
      "source": [
        "## Step 1. Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qpkDbkUVJEW",
        "colab_type": "text"
      },
      "source": [
        "First step is prepearing the data. Here's the explaination of the whole process:\n",
        "1. Fetch the data from the URL using `urlopen`\n",
        "2. Read the fetched data\n",
        "3. Using `BeautifulSoup`, parse the data and collect the article (usually found in `<p>` tag)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFmo-Z6HVLGi",
        "colab_type": "code",
        "outputId": "19c06046-3e1d-4e56-e0ee-404b15d4464b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import bs4\n",
        "import urllib.request\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# only when needed\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Fetch the content from the URL\n",
        "url = 'https://www.theverge.com/circuitbreaker/2019/9/18/20868935/google-pixel-4-xl-rumors-leaks-specs-details-colors-cameras-soli'\n",
        "fetched_data = urllib.request.urlopen(url)\n",
        "\n",
        "article_read = fetched_data.read()\n",
        "\n",
        "# Parsing the URL content and storing in a variable\n",
        "soup = bs4.BeautifulSoup(article_read, 'html.parser')\n",
        "\n",
        "article_content = []\n",
        "\n",
        "for element in soup.find_all('p'):\n",
        "    article_content.append(element.text)\n",
        "    \n",
        "article_content = ' '.join(article_content)\n",
        "article_content = sent_tokenize(article_content)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAUdHlBFVMV4",
        "colab_type": "text"
      },
      "source": [
        "Below are the first 5 sentences from the article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpaPlVY6VRhc",
        "colab_type": "code",
        "outputId": "dab7b272-a726-41d6-bf53-6c72a188e09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "article_content[:5]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Filed under: It’s coming October 15th Google’s Pixel 3 might have been the most-leaked phone in history.',\n",
              " 'Long before its unveiling, we knew practically everything about it from unboxing videos, photo comparisons, even a full review of the device.',\n",
              " 'Surely, for the Pixel 4, Google would clamp down on leaks to leave some surprises for its debut on October 15th, right?',\n",
              " 'Let’s just say, if that was the plan, it didn’t exactly work out.',\n",
              " 'We’ve now seen the Pixel 4 XL from every angle and in three different colors.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzfYxE28VdMx",
        "colab_type": "text"
      },
      "source": [
        "## Step 2. Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utAlgPGpVf2t",
        "colab_type": "text"
      },
      "source": [
        "Next, we will create a frequency table, but first we need a cleaned version of the article content (which mean we have to lemmatize the content and remove the stop words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LInkmVxVmB5",
        "colab_type": "code",
        "outputId": "2cc13067-630e-4a87-dcaf-8648a52ea05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "import string\n",
        "\n",
        "# only when needed\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "clean_data = []\n",
        "\n",
        "def preprocessing(text):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    \n",
        "    result = []\n",
        "    for token in nltk.word_tokenize(text):\n",
        "        root = lemmatizer.lemmatize(token)\n",
        "        if root in string.punctuation: continue\n",
        "        if root in stopwords: continue\n",
        "        result.append(root)\n",
        "    \n",
        "    return ' '.join(result)\n",
        "    \n",
        "for sent in article_content:\n",
        "    clean_data.append(preprocessing(sent))\n",
        "    \n",
        "clean_data[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['filed ’ coming october 15th google ’ pixel 3 might most-leaked phone history',\n",
              " 'long unveiling knew practically everything unboxing video photo comparison even full review device',\n",
              " 'surely pixel 4 google would clamp leak leave surprise debut october 15th right',\n",
              " 'let ’ say wa plan ’ exactly work',\n",
              " '’ seen pixel 4 xl every angle three different color']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JCb4q1nVo8b",
        "colab_type": "text"
      },
      "source": [
        "After cleaning the data, next step we can make a frequency table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgaP8f4cV-z7",
        "colab_type": "code",
        "outputId": "d4db5c56-9999-495d-955f-b73f7a476a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(clean_data)\n",
        "vectors"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<121x650 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1445 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2hczkBKWBYq",
        "colab_type": "text"
      },
      "source": [
        "As you can see size of the matrix is $121\\times650$, where each rows represent each sentences and the columns represent the number of word occurance. Below are the top 5 words with the highest frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYeSSTnqWEce",
        "colab_type": "code",
        "outputId": "27409fd5-47cf-465f-ffa4-afdf6aa06d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "res = np.sum(vectors.toarray(), axis=0)\n",
        "vocab = vectorizer.vocabulary_\n",
        "freq_table = dict()\n",
        "\n",
        "for word, ix in vocab.items():\n",
        "    freq_table[word] = res[ix]\n",
        "\n",
        "N = len(freq_table.values())\n",
        "\n",
        "for word in freq_table:\n",
        "    freq_table[word] /= N\n",
        "\n",
        "sorted(freq_table.items(), key=lambda x: x[1], reverse=True)[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pixel', 0.1076923076923077),\n",
              " ('google', 0.06153846153846154),\n",
              " ('phone', 0.05076923076923077),\n",
              " ('know', 0.04923076923076923),\n",
              " ('camera', 0.036923076923076927)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWlWVhUJWIcw",
        "colab_type": "text"
      },
      "source": [
        "As we can expect, the article is about the Pixel 4, a new phone from Google, so the result is suitable with the article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDtjqXgnWxVh",
        "colab_type": "text"
      },
      "source": [
        "## Step 3. Finding the weighted frequencies of the sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPIfL5k8Xd71",
        "colab_type": "text"
      },
      "source": [
        "Now, we already have the tokenize version of the article and the frequency table for each token. Pretty much can now begin to implement the algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_DRIAVsX_ec",
        "colab_type": "code",
        "outputId": "74048e71-35ae-43ff-ccab-5072379bb083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sent_weight = dict()\n",
        "\n",
        "for ix, sent in enumerate(clean_data):\n",
        "    \n",
        "    list_word = word_tokenize(sent)\n",
        "    g_sent = 0\n",
        "    \n",
        "    for word in list_word:\n",
        "        if word in freq_table:\n",
        "            g_sent += freq_table[word]\n",
        "            \n",
        "    sent_weight[ix] = g_sent / len(list_word)\n",
        "    \n",
        "top5 = sorted(sent_weight.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "top5"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(54, 0.04246153846153846),\n",
              " (25, 0.031153846153846157),\n",
              " (26, 0.029230769230769237),\n",
              " (36, 0.02923076923076923),\n",
              " (46, 0.026153846153846153)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx6r1wKCbjs8",
        "colab_type": "text"
      },
      "source": [
        "## Step 4. Getting the summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqv1Gna2ZKUs",
        "colab_type": "text"
      },
      "source": [
        "It's turn out the 55th sentence is the best pick to summarize the article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN3aJM_ybOqG",
        "colab_type": "code",
        "outputId": "3440997a-8d05-4bd5-e189-6170f6da45a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "article_content[54]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What we know: The Pixel is no longer a single-camera phone.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM6UB2TFbQM8",
        "colab_type": "text"
      },
      "source": [
        "But for sake of curiosty, let's looks the top 5 sentence!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5zlJ9q-btF-",
        "colab_type": "code",
        "outputId": "e0791d1e-6064-44b5-e31e-61fc9a4120fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for ix, _ in top5:\n",
        "    print(article_content[ix])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What we know: The Pixel is no longer a single-camera phone.\n",
            "But the Pixel 3, Pixel 3 XL and Pixel 3A look close enough that, honestly, we doubt it.\n",
            "What we know: The Pixel 4 will come in black and orange.\n",
            "What we know: Not much.\n",
            "What we know: Nothing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWuwnBGcs7t",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvLid7gpb5cO",
        "colab_type": "text"
      },
      "source": [
        "I've to admit that the result is good so far! The article is about the leaked-information on Pixel 4. It's turn out that the writer is not quite sure if he know much about the specs despite having those information. Of course, we have to look at the article by ourselves if we want to make sure if this summary is satisfying. But I leave the homework for you, the reader, to comment about those result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae_tU7N_cgLb",
        "colab_type": "text"
      },
      "source": [
        "## Reference\n",
        "\n",
        "This project is nothing without this awesome source:\n",
        "1. *Text Summarization Techniques: A Brief Survey* - https://arxiv.org/pdf/1707.02268\n",
        "2. *Beyond SumBasic: Task-Focused Summarization with Sentence Simplification and Lexical Expansion* - https://www.cis.upenn.edu/~nenkova/papers/ipm.pdf\n",
        "3. *Applied Text Analysis with Python* - https://learning.oreilly.com/library/view/applied-text-analysis/9781491963036/\n",
        "4. *Natural Language Processing with Python* - http://www.nltk.org/book/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FymkC9Hcc7xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}